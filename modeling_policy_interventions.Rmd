---
title: "Modeling policy interventions"
subtitle: "with applications in terraforming"
author: "Kyle D Hart"
date: "18 November 2020"
output:
  ioslides_presentation:
    css: gfx/styles.css
    logo: gfx/logo_gray.png
    toc: yes
    keep_md: yes
widescreen: yes
  
---

<style>
.forceBreak { -webkit-column-break-after: always; break-after: column; }
</style>

```{r setup, include=FALSE}
# library(multcomp)
#library(trahelyk)
library(dplyr)
library(magrittr)
library(broom)
library(stringr)
library(purrr)
library(grid)
library(xkcd)
library(ggdag)
library(latex2exp)
library(tictoc)
library(tidyr)
library(estimatr)
library(sandwich)
library(lmtest)
knitr::opts_chunk$set(echo = FALSE,
                      warning=FALSE,
                      fig.width=10)
```

# Intro

## Agenda

* Intro
* One-sample methods:
  - Pre/post means
  - Interrupted time series
* Two-sample methods: 
  - Treatment/comparison means
  - Comparative interrupted time series
  - Difference in differences
* Regression methods
  - Fixed effects
  - Linear probabilty models


<div class="notes">
This is my *note*.
- It can contain markdown
- like this list
</div>

## About me, about this talk

* Not an authority
* Been thinking about this stuff
* It's been weird having a biostats background and then jumping in with all these economist-types.
* They probably assumed I was 100% on board with all this econometrics stuff when they asked me to do this HAHAHAAHAa

## Estimate, estimator, estimand

* I tried to frame this similar to https://diff.healthpolicydatascience.org/. You should read that if you haven't already. 
* For each approach, think about the estimand you are trying to estimate.

## Notation

$$
\begin{align}
  Y(t) ~~&|~~  \mbox{Observed outcome at time } t \\
  D = 0 ~~&|~~ \mbox{Control} \\
  D = 1 ~~&|~~ \mbox{Treated} \\
  t = 1, \ldots, T_0 ~~&|~~ \mbox{Pre-intervention times} \\
  t = T_0 + 1, \ldots, T ~~&|~~ \mbox{Post-intervention times} \\
  Y^d(t) ~~&|~~ \mbox{Potential outcome with treatment } D=d \mbox{ at time } t
\end{align}
$$
<!-- X ~~&|~~ \mbox{Observed covariates} \\ -->
<!--   U ~~&|~~ \mbox{Unobserved covariates} -->

## Counterfactual framework

**Two parallel universes** ![](gfx/parallel.jpg){width=30%, height=30%, align=right}

* one where the intervention occurred 
* one where the intervention did not occur

<br><br>

Policy effect = difference between the outcomes in those two universes

## Average treatment effect on the treated (ATT)

**Estimand:**
$$
\begin{align}
ATT =& E[Y^1 - Y^0 | D = 1] \\
    =& E[Y^1 | D = 1] - E[Y^0 | D = 1]
\end{align}
$$


**Estimator:**

$$
\begin{align}
ATT &\approx E[Y^1 | D = 1] - \mbox{(some surrogate for } E[Y^0 | D = 1])
\end{align}
$$


# Application: Terraforming LV-426

## Hadley's Hope
![LV-426](gfx/acheron_full.jpg)

## Building Better Worlds | The Weyland-Yutani workplace safety program

* Terraforming workers at the Hadley's Hope colony on exomoon LV-426 sometimes experience serious workplace injuries. 
* These injuries can lead to expensive claims against the company and delays in the colonization schedule. 
* On October 1, 2176, the Weyland-Yutani corporation implemented a safety program that included a mandatory 3-hour training workshop and disciplinary measures for workers who don't follow safety protocols. 

## Data

```{r}
intervention <- 31
set.seed(9583)
colonies <- tibble(colony = rep(c("Hadley's Hope",
                                  "Relitor",
                                  "Arceon",
                                  "Articus",
                                  "Arcturus",
                                  "Fiorina 161",
                                  "Argos",
                                  "Atlas",
                                  "Crysalis",
                                  "Cyrus"), each = 60),
                   time = rep(1:60, times=10),
                   t = rep(-30:29, times=10),
                   colony_effect = rep(rnorm(n=10, mean=0, sd=10), each=60)) %>%
  mutate(colony_effect = case_when(colony=="Fiorina 161" ~ 25,
                                   TRUE ~ colony_effect)) %>%
  mutate(weather_index = runif(n=nrow(.), min=0, max=10),
         treat = rep(c(1, 0), each=60*5),
         post = as.numeric(time > intervention)) %>% 
  mutate(y = 500 + (3 * time) + 
           (20 * treat) + 
           (-40 * treat * post) + 
           (-1 * treat * post * (time-intervention)) + 
           (5 * weather_index) +
           colony_effect + 
           rnorm(n=nrow(.), mean=0, sd=20))

colonies$colony_effect[colonies$colony=="Fiorina 161"] <- -18

hh <- colonies %>% filter(colony=="Hadley's Hope")

hhf <- colonies %>% filter(colony %in% c("Hadley's Hope", "Fiorina 161")) %>%
  mutate(colony_tx = factor(treat, levels=c(0, 1),
                            labels=c("Fiorina 161 --comparison--",
                                     "Hadley's Hope --intervention--")))

head(hh)
```

## Broad research question

> What effect does this policy intervention have on monthly operating costs?  

## Some strategies

* Pre/post test
* Interrupted time series (ITS)
* Comparative ITS
* Difference in differences


# Comparing pre/post means

## Operating costs over time: <span style="color:red">Panel data</span>

```{r}
(hh_proto <- ggplot(data=hh,
                   aes(x=time, y=y)) +
  geom_point(alpha=0.5, size=2.5) +
  annotate(geom="text",
           x = 0,
           y = min(hh$y) - 8, 
           label=" Start of terraforming operation",
           hjust=0, 
           family = "xkcd") +
  annotate(geom="text",
           x = intervention,
           y = min(hh$y) - 8, 
           label=" Month 31: Implement safety program",
           hjust=0, 
           family = "xkcd") +
  geom_vline(xintercept = 31) +
  scale_y_continuous(name="Monthly operating costs, millions") +
  scale_x_continuous(breaks=seq(0, 60, 10)) +
  xkcdaxis(range(hh$time), range(hh$y)) +
  theme(legend.title = element_blank(),
        legend.position = "bottom"))

```


## Compare means

```{r}
xbar_pre <- mean(hh$y[hh$post==0])
xbar_post <- mean(hh$y[hh$post==1])
delta <- round(xbar_post - xbar_pre)

hh_proto + 
  xkcdline(data = tibble(x1 = c(1, 32),
                         x2 = c(30, 59),
                         y1 = c(xbar_pre, xbar_post),
                         post = factor(c("Pre", "Post"))),
           aes(x=x1, xend=x2,
               y=y1, yend=y1,
               colour = post),
           mask=FALSE) +
  annotate(geom="text",
           x = 15, y = xbar_pre + 11,
           label = paste("bar(x)[pre] ==", round(xbar_pre, 0)),
           parse=TRUE,
           family="xkcd", size=10, color="#00BFC4") + 
  annotate(geom="text",
           x = 45, y = xbar_post + 11,
           label = paste("bar(x)[post] ==", round(xbar_post, 0)),
           parse=TRUE,
           family="xkcd", size=10, color="#F8766D") + 
  xkcdline(data = tibble(x = 31.5, xend = 31.5,
                         y = round(xbar_pre),
                         yend = round(xbar_post)),
           aes(x=x, xend=xend,
               y=y, yend=yend),
           mask=FALSE) +
  xkcdline(data = tibble(x = 31.5 - 0.5, xend = 31.5 + 0.5,
                         y = round(xbar_pre),
                         yend = round(xbar_pre)),
           aes(x=x, xend=xend,
               y=y, yend=yend),
           mask=FALSE) +
  xkcdline(data = tibble(x = 31.5 - 0.5, xend = 31.5 + 0.5,
                         y = round(xbar_post),
                         yend = round(xbar_post)),
           aes(x=x, xend=xend,
               y=y, yend=yend),
           mask=FALSE) +
  xkcdline(data = tibble(x = 31.5, xend = 33.5,
                         y = xbar_pre + (delta/2),
                         yend = xbar_pre + (delta/2)),
           aes(x=x, xend=xend,
               y=y, yend=yend),
           mask=FALSE) +
    annotate(geom="text",
           x = 34,
           y = xbar_pre + (delta/2),
           label= paste("delta ==", delta),
           parse=TRUE,
           hjust=0, size=10,
           family = "xkcd") +
  scale_color_discrete(c("red", "blue")) +
  theme(legend.title = element_blank(),
        legend.position = "none") 
```

## ATT

$$
\begin{align}
ATT =& E[Y^1 - Y^0 | D = 1] \\
    =& E[Y^1 | D = 1] - E[Y^0 | D = 1]
\end{align}
$$
$E[Y^0 | D = 1]$ is in the other universe, so assume 

$$
\begin{align}
E[Y^0 | D = 1] &\approx E[Y^1(t \le T_0) | D = 1]
\end{align}
$$

## Counterfactual

```{r}
hh_proto + 
  xkcdline(data = tibble(x1 = c(1, 32),
                         x2 = c(30, 59),
                         y1 = c(xbar_pre, xbar_post),
                         post = factor(c("Pre", "Post"))),
           aes(x=x1, xend=x2,
               y=y1, yend=y1,
               colour = post),
           mask=FALSE, lwd=1.5) +
  geom_line(data = tibble(x = c(32, 59), 
                         y = c(xbar_pre, xbar_pre)),
           aes(x=x, 
               y=y),
           lwd=1.5, lty=2, colour = "darkgrey") + 
  annotate(geom="text",
           x = 15, y = xbar_pre + 11,
           label = TeX("$E \\lbrack Y^1(t \\leq T_0) | D = 1 \\rbrack $"),
           size=10, color="#00BFC4") +
  annotate(geom="text",
           x = 45, y = xbar_post + 11,
           label = TeX("$E \\lbrack Y^1(t > T_0) | D = 1 \\rbrack $"),
           size=10, color="#F8766D") + 
  annotate(geom="text",
           x = 45, y = xbar_pre + 11,
           label = TeX("$E \\lbrack Y^0(t > T_0) | D = 1 \\rbrack $"),
           size=10, color="darkgrey") + 
  scale_color_discrete(c("red", "blue")) +
  theme(legend.title = element_blank(),
        legend.position = "none") 
```




## ATT

Then 

$$
\begin{align}
ATT &= E[Y^1 - Y^0 | D = 1] \\
 &= E[Y^1 | D = 1] - E[Y^0 | D = 1] \\
 &= E[Y^1(t > T_0) | D = 1] - E[Y^0(t > T_0) | D = 1] \\
 &= E[Y^1(t > T_0) | D = 1] - E[Y^1(t < T_0) | D = 1] \\
 & = \bar{X}_{t > T_0} - \bar{X}_{t < T_0}
\end{align}
$$

## t-test
$$
t = \frac{\bar{X}_{post} - \bar{X}_{pre}}{s \sqrt{\frac{2}{n}}}
$$

```{r echo=TRUE, results='hold'}
mean(hh$y[hh$post==1]) - mean(hh$y[hh$post==0])
with(hh, t.test(y ~ post))
```

## OLS

Similarly, 
$$
Y = \beta_0 + \beta_1 \mbox{POST} + \mathbf{\beta_x X} + \varepsilon
$$

```{r echo=TRUE}
lm_prepost <- lm(y ~ post + weather_index,
           data=hh)
```

```{r}
tidy(lm_prepost) %>% 
  mutate(p.value = round(p.value, 3))
```

## Assumptions

<!-- **Assumptions:** -->

* Outcomes would stay the same over time in the absence of the intervention
* No unmeasured variables affecting the outcome

<!-- **Accounts for:** -->

<!-- * Baseline value for outcomes -->

<!-- **Does not account for:** -->

<!-- * Outcome was already getting better or worse -->
<!-- * Anything that changed outcomes unrelated to the intervention -->


# Interrupted time series

## Operating costs over time

```{r}
hh_proto
```

## Change in level

```{r}
lm_its <- lm(y ~ time + post,
             data = hh)
hh_proto +
  geom_smooth(data = hh %>% filter(post==0) %>%
                mutate(y = predict(lm_its, 
                                   newdata=.)),
              method="lm", formula="y ~ x") +
  geom_smooth(data = tibble(post = rep(0, 29),
                            time = 32:60) %>%
                mutate(y = predict(lm_its, 
                                   newdata=.)),
              method="lm", formula="y ~ x",
              lty=2) + 
  geom_smooth(data = hh %>% filter(post==1) %>%
                mutate(y = predict(lm_its, 
                                   newdata=.)),
              method="lm", formula="y ~ x")

```

## Change in level

$$
\begin{align}
\mbox{Compare means:} ~~|&~~ Y = \beta_0 + \beta_1 POST + \varepsilon \\
\mbox{ITS (level):} ~~|&~~ Y_{t} = \beta_0 + \beta_1 \mbox{TIME} + \beta_2 \mbox{POST} + \varepsilon_t\\
\end{align}
$$

$$
\begin{align}
ATT &= E[Y^1 - Y^0 | D = 1] \\
 &= E[Y^1 | D = 1] - E[Y^0 | D = 1] \\
 & = \hat{\beta}_2
\end{align}
$$

## Linear model

```{r echo=TRUE}
lm_its <- lm(y ~ t + post,
             data = hh)
```

```{r}
(coefs_its <- tidy(lm_its) %>% 
  mutate(p.value = round(p.value, 3)))

```

## Change in level and slope

```{r}
lm_its2 <- lm(y ~ time*post,
             data = hh)
hh_proto +
  # Predicted pre-intervention
  geom_smooth(data = hh %>% filter(post==0) %>%
                mutate(y = predict(lm_its2, 
                                   newdata=.)),
              method="lm", formula="y ~ x") +
  # Predicted post-intervention counterfactual
  geom_smooth(data = tibble(post = rep(0, 29),
                            time = 32:60) %>%
                mutate(y = predict(lm_its2, 
                                   newdata=.)),
              method="lm", formula="y ~ x",
              lty=2) + 
  # Predicted post-intervention observed
  geom_smooth(data = hh %>% filter(post==1) %>%
                mutate(y = predict(lm_its2, 
                                   newdata=.)),
              method="lm", formula="y ~ x")

```

## Change in level and slope | Linear model

$$
\begin{align}
\mbox{Compare means:} ~~|&~~ Y = \beta_0 + \beta_1 POST + \varepsilon \\
\mbox{ITS (level):} ~~|&~~ Y_{t} = \beta_0 + \beta_1 \mbox{TIME} + \beta_2 \mbox{POST} + \varepsilon_t\\
\mbox{ITS (level & slope):} ~~|&~~ Y_{t} = \beta_0 + \beta_1 \mbox{TIME} + \beta_2 \mbox{POST} + \beta_3 \mbox{TIME} \times \mbox{POST} + \varepsilon_t\\
\end{align}
$$

## ATT

$$
Y_{t} = \beta_0 + \beta_1 \mbox{TIME} + \beta_2 \mbox{POST} + \beta_3 \mbox{TIME} \times \mbox{POST} + \varepsilon_t
$$

So for any $t \in T \ge T_0$,

$$
\begin{align}
ATT(t) &= E[Y^1(t) - Y^0(t) | D = 1] \\
 &= E[Y^1(t) | D = 1] - E[Y^0(t) | D = 1] \\
 & = \hat{\beta}_2 + (\hat{\beta}_3 \times t)
\end{align}
$$

## Change in level and slope | Linear model
```{r echo=TRUE}
lm_its2 <- lm(y ~ t*post,
             data = hh)
```

```{r}
(coefs_its2 <- tidy(lm_its2) %>% 
  mutate(p.value = round(p.value, 3)))
```

## Interrupted time series

**Assumes** the observations from before the intervention are a reasonable surrogate for the counterfactual condition of the test group to represent what would have happened without an intervention.

**Accounts for** outcomes that were already getting better or worse

**Does not account for** anything unrelated to the intervention that changed outcomes in the post-intervention period

## <span style="color:red">Exogenous</span> variables? {.columns-2}

![P-5000 Powered Work Loader](gfx/p5000.jpg){width=400}

<p class="forceBreak"></p>

```{r warning=FALSE, message=FALSE, out.width="470px"}
set.seed(3459)
tibble(Cost = rnorm(n=12, mean = 50000, sd=5000),
       Month = seq(1, 60, by=5)) %>%
  ggplot(aes(x=Month, y=Cost)) +
  # geom_point() + 
  # geom_line() +
  geom_smooth(se=FALSE) +
  scale_y_continuous(limits=c(46000, 52000)) +
    theme(legend.title = element_blank(),
        legend.position = "none",
        axis.title = element_text(size=30),
        axis.text = element_text(size=15 )) 
```


What if there's some external force causing costs to fluctuate over time? 

# Comparative methods

## Fiorina 161
![Fiorina 161, Outer Veil](gfx/fiorina161.jpg){width=100%, height=90%}

## Fiorina 161 and Hadley's Hope
```{r}
(hhf_proto <- ggplot(data=hhf,
       aes(x=time, y=y)) +
  geom_point(aes(group=colony, color=colony),
             alpha=0.4, size=2.5) +
    annotate(geom="text",
           x = 0,
           y = min(hhf$y) - 11, 
           label=" Start of terraforming operation",
           hjust=0, 
           family = "xkcd") +
    annotate(geom="text",
           x = intervention,
           y = min(hhf$y) - 11, 
           label=" Month 31: Implement safety program",
           hjust=0, 
           family = "xkcd") +
  geom_vline(xintercept = 31) +
  scale_y_continuous(name="Monthly operating costs, millions") +
  scale_x_continuous(breaks=seq(0, 60, 10)) +
  scale_color_discrete(c("red", "blue")) + 
  xkcdaxis(range(hhf$time), range(hhf$y)) +
  theme(legend.title = element_blank(),
        legend.box.background = element_rect(colour = "black"),
        legend.position = c(0.2, 0.8)))
```

## Two-group post-mean comparison

```{r}
xbar_0 <- mean(hhf$y[hhf$treat==1 & hhf$post==0])
xbar_1 <- mean(hhf$y[hhf$treat==1 & hhf$post==1])
delta <- round(xbar_1) - round(xbar_0)

hhf_proto +
  xkcdline(data = tibble(x1 = c(32),
                         x2 = c(59),
                         y1 = c(xbar_1),
                         post = factor(c("Hadley's Hope"))),
           aes(x=x1, xend=x2,
               y=y1, yend=y1,
               colour = post),
           mask=FALSE) +
  annotate(geom="text",
           x = 15, y = xbar_0 + 12,
           label = paste("bar(x)['Hadleys NOPE'] ==", round(xbar_0, 0)),
           parse=TRUE,
           family="xkcd", size=10, color="black") + 
  annotate(geom="text",
           x = 45, y = xbar_1 - 12,
           label = paste("bar(x)['Hadleys Hope'] ==", round(xbar_1, 0)),
           parse=TRUE,
           family="xkcd", size=10, color="#00BFC4") + 
  xkcdline(data = tibble(x = 2 - 0.5, xend = 30 + 0.5,
                         y = round(xbar_0)-2,
                         yend = round(xbar_0)-2),
           aes(x=x, xend=xend,
               y=y, yend=yend),
           mask=FALSE)

```

## Two-group post-mean comparison

```{r}
xbar_0 <- mean(hhf$y[hhf$treat==0 & hhf$post==1])
xbar_1 <- mean(hhf$y[hhf$treat==1 & hhf$post==1])
delta <- round(xbar_1) - round(xbar_0)

hhf_proto +
  xkcdline(data = tibble(x1 = c(32, 32),
                         x2 = c(59, 59),
                         y1 = c(xbar_0, xbar_1),
                         post = factor(c("Fiorina 161", "Hadley's Hope"))),
           aes(x=x1, xend=x2,
               y=y1, yend=y1,
               colour = post),
           mask=FALSE) +
  annotate(geom="text",
           x = 45, y = xbar_0 + 12,
           label = paste("bar(x)['Fiorina 161'] ==", round(xbar_0, 0)),
           parse=TRUE,
           family="xkcd", size=10, color="#F8766D") + 
  annotate(geom="text",
           x = 45, y = xbar_1 - 12,
           label = paste("bar(x)['Hadleys Hope'] ==", round(xbar_1, 0)),
           parse=TRUE,
           family="xkcd", size=10, color="#00BFC4") + 
  xkcdline(data = tibble(x = 45.5, xend = 45.5,
                         y = round(xbar_0) - 2,
                         yend = round(xbar_1) + 2),
           aes(x=x, xend=xend,
               y=y, yend=yend),
           mask=FALSE) +
  xkcdline(data = tibble(x = 45.5 - 0.5, xend = 45.5 + 0.5,
                         y = round(xbar_0)-2,
                         yend = round(xbar_0)-2),
           aes(x=x, xend=xend,
               y=y, yend=yend),
           mask=FALSE) +
  xkcdline(data = tibble(x = 45.5 - 0.5, xend = 45.5 + 0.5,
                         y = round(xbar_1)+2,
                         yend = round(xbar_1)+2),
           aes(x=x, xend=xend,
               y=y, yend=yend),
           mask=FALSE) +
    annotate(geom="text",
           x = 46,
           y = xbar_0 + (delta/2),
           label= paste("delta ==", delta),
           parse=TRUE,
           hjust=0, size=10,
           family = "xkcd") 

```

## Two-group post-mean comparison

$$
t = \frac{\bar{X}_{test} - \bar{X}_{comparison}}{s \sqrt{\frac{2}{n}}}
$$

```{r echo=TRUE}
mean(hhf$y[hhf$treat==1]) - mean(hhf$y[hhf$treat==0])
with(hhf, t.test(y ~ post))
```

## Two-group post-mean comparison

$$
\begin{align}
\mbox{Test/comparison post-means:} ~~|&~~ Y = \beta_0 + \beta_1 \mbox{TREAT} + \mathbf{\beta_x X} + \varepsilon
\end{align}
$$

```{r echo=TRUE}
lm_2grp <- lm(y ~ treat + weather_index,
           data=hhf %>% filter(post==1))
```

```{r}
print(tidy(lm_2grp) %>% 
        mutate(p.value = round(p.value, 3)))
```

## Two-group post-mean comparison

* **Assumes** that the comparison group is a good representation of the counterfactual $E[Y^0 | D=1]$; specifically, that outcomes for the two groups would be the same if neither or both had received the intervention

* **Accounts for** anything unrelated to the intervention that changed outcomes equally for both groups

* **Does not account for** any baseline differences in levels or slopes between the two groups or non-intervention <span style="color:red">shocks</span> that affected only one group

# Comparative interrupted time series

## Intuition

```{r}
lm_cits <- lm(y ~ time*post + time*treat + treat*post + I(treat * post * time),
             data = hhf)

hhf_proto +
  # Predicted pre-intervention
  geom_smooth(data = hhf %>% filter(post==0) %>%
                mutate(y = predict(lm_cits, 
                                   newdata=.)),
              aes(group=colony, color=colony),
              method="lm", formula="y ~ x") +
  # Predicted post-intervention
  geom_smooth(data = hhf %>% filter(post==1) %>%
                mutate(y = predict(lm_cits, 
                                   newdata=.)),
              aes(group=colony, color=colony),
              method="lm", formula="y ~ x") 
  

```

## 

$$
\begin{align}
\mbox{ITS (level):} ~~|&~~ Y_{t} = \beta_0 + \beta_1 \mbox{TIME} + \beta_2 \mbox{POST} + \varepsilon_t\\
\mbox{ITS (level & slope):} ~~|&~~ Y_{t} = \beta_0 + \beta_1 \mbox{TIME} + \beta_2 \mbox{POST} + \beta_3 \mbox{TIME} \times \mbox{POST} + \varepsilon_t\\
\end{align}
$$

$$
\begin{align}
  Y_t =& \beta_0 + \beta_1 \mbox{TIME} + \beta_2 \mbox{POST} + \beta_3 \mbox{POST} \times \mbox{TIME} + \\
     & \beta_4 \mbox{TREAT} + \beta_5 \mbox{TREAT} \times \mbox{TIME } + \\
     & \beta_6 \mbox{TREAT} \times \mbox{POST} + \beta_7 \mbox{TREAT} \times \mbox{POST} \times \mbox{TIME} + \varepsilon_t
\end{align}
$$

where
$$
\mbox{TIME } = \mbox{ {1, ... 60}},\\
\mbox{POST } = \begin{cases}
                0 \mbox{ if TIME} \le 31, \\
                1 \mbox{ if TIME > 31}\\
              \end{cases},\\
\mbox{TREAT } = \begin{cases}
                0 \mbox{ = Fiorina 161}, \\
                1 \mbox{ = Hadley's Hope}
              \end{cases}
$$

##

$$
\begin{align}
  Y_t =& \beta_0 + \beta_1 \mbox{TIME} + \beta_2 \mbox{POST} + \beta_3 \mbox{POST} \times \mbox{TIME} + \\
     & \beta_4 \mbox{TREAT} + \beta_5 \mbox{TREAT} \times \mbox{TIME } + \\
     & \beta_6 \mbox{TREAT} \times \mbox{POST} + \beta_7 \mbox{TREAT} \times \mbox{POST} \times \mbox{TIME} + \varepsilon_t
\end{align}
$$

<center><img src="gfx/fh_icon.png" height="30px" /></center>

$$ 
\begin{align}
  ATT(t) = & E[Y^1 | D = 1](t) - E[Y^0 | D = 1](t) \\
  E[Y^1 | D = 1](t) =&  (\beta_0 + \beta_1 t + \beta_2 + \beta_3 t + \beta_4 + \beta_5 t + \beta_6 + \beta_7 t) - \\
  & (\beta_0 + \beta_1 t + \beta_4 + \beta_5 t) \\ 
  =& \beta_2 + \beta_3 t + \beta_6 + \beta_7 t \\
  E[Y^0 | D = 1] =& (\beta_0 + \beta_1 t + \beta_2 + \beta_3 t) - (\beta_0 + \beta_1 t) \\
  =& \beta_2 + \beta_3 t \\
  E[Y^1 | D = 1] - E[Y^0 | D = 1] =& \beta_6 + \beta_7 t
\end{align}
$$


## Model

```{r echo=TRUE}
lm_cits <- lm(y ~ t*post + t*treat + treat*post + I(treat * post * t),
             data = hhf)
```

```{r echo=FALSE}
(coefs_cits <- tidy(lm_cits) %>% 
  mutate(p.value = round(p.value, 3)))
```

## Linear combination

```{r echo=TRUE}
summary(multcomp::glht(lm_cits, 
             linfct = matrix(c(0, 0, 0, 0, 12, 0, 0, 1), 1)))
```


# Difference in differences

## DiD: Intuition

```{r}
lm_did <- lm(y ~ treat*post,
             data=hhf)

xbar_pre0 <- mean((hhf %>% filter(post==0 & treat==0) %>%
            mutate(y = predict(lm_did, 
                               newdata=.)) %>%
             pull(y)))

xbar_pre1 <- mean((hhf %>% filter(post==0 & treat==1) %>%
            mutate(y = predict(lm_did, 
                               newdata=.)) %>%
             pull(y)))

xbar_post0 <- mean((hhf %>% filter(post==1 & treat==0) %>%
            mutate(y = predict(lm_did, 
                               newdata=.)) %>%
             pull(y)))

xbar_post1 <- mean((hhf %>% filter(post==1 & treat==1) %>%
            mutate(y = predict(lm_did, 
                               newdata=.)) %>%
             pull(y)))

hhf_proto +
  # Predicted pre-intervention
  geom_smooth(data = hhf %>% filter(post==0) %>%
                mutate(y = predict(lm_did, 
                                   newdata=.)),
              aes(group=colony, color=colony),
              method="lm", formula="y ~ x") +
  # Predicted post-intervention
  geom_smooth(data = hhf %>% filter(post==1) %>%
                mutate(y = predict(lm_did, 
                                   newdata=.)),
              aes(group=colony, color=colony),
              method="lm", formula="y ~ x") +
  geom_line(data = tibble(x = c(31, 31.5, 31.5, 32, 31, 33, 33), 
                         y = c(xbar_pre0, xbar_pre0, xbar_post0, xbar_post0, 
                               xbar_pre1, xbar_pre1, xbar_post1),
                         colony = c(rep("Fiorina 161", 4), rep("Hadley's Hope", 3))),
           aes(x=x, 
               y=y, 
               colour = colony),
           lty=2) + 
  annotate(geom="text",
           x = 33.5, y = 600,
           label = "delta['d=1']",
           parse=TRUE,
           hjust=0,
           family="xkcd", size=10, color="#00BFC4") +
  annotate(geom="text",
           x = 32, y = 640,
           label = "delta['d=0']",
           parse=TRUE,
           hjust=0,
           family="xkcd", size=10, color="#F8766D") +
  annotate(geom="text",
           x = 35, y = 550,
           label = "'DiD' == delta['d=1'] - delta['d=0']",
           parse = TRUE, hjust=0,
           family = "xkcd", size=12)
  
```

## ATT

$$
\begin{align}
  ATT =& E\left[\bar{Y^1}_{t > T_0} - \bar{Y^1}_{t \le T_0} | D = 1 \right] - E\left[\bar{Y^0}_{t > T_0} - \bar{Y^0}_{t \le T_0} | D = 1 \right]
\end{align}
$$

Counterfactual assumption

$$
\begin{align}
  E\left[\bar{Y^0}_{t > T_0} - \bar{Y^0}_{t \le T_0} | D = 1 \right] &\approx E\left[\bar{Y^0}_{t > T_0} - \bar{Y^0}_{t \le T_0} | D = 0 \right] \\
\end{align}
$$
Then

$$
\begin{align}
  ATT &\approx E\left[\bar{Y^1}_{t > T_0} - \bar{Y^1}_{t \le T_0} | D = 1 \right] - E\left[\bar{Y^0}_{t > T_0} - \bar{Y^0}_{t \le T_0} | D = 0 \right] \\ 
\end{align}
$$

## A crude DiD

```{r echo = FALSE, results="asis"}
hhf %<>%
  mutate(t_f = factor(t),
         tx_post = treat==1 & post==1)

fiorina <- hhf %>% filter(colony=="Fiorina 161")
hadley <-  hhf %>% filter(colony=="Hadley's Hope")

fiorina_pre <- mean(fiorina %>% filter(post==0) %>% pull(y))
fiorina_post <- mean(fiorina %>% filter(post==1) %>% pull(y))
fiorina_diff <- fiorina_post - fiorina_pre

hadley_pre <- mean(hadley %>% filter(post==0) %>% pull(y))
hadley_post <- mean(hadley %>% filter(post==1) %>% pull(y))
hadley_diff <- hadley_post - hadley_pre

print(tibble(Colony = c("Fiorina 161 (comparison)", "Hadley's Hope (test)"),
       Pre = round(c(fiorina_pre, hadley_pre)),
       Post = round(c(fiorina_post, hadley_post))) %>%
  mutate(Difference = Post - Pre))

```

<br>

```{r echo=TRUE}
31-93
```


## Linear model

$$
\begin{align}
Y &= \beta_0 + \beta_1 \mbox{TREAT} + \beta_2 \mbox{POST} + \\
  & ~~~~~~~~~~~~~\beta_3 \mbox{TREAT} \times \mbox{POST} + \varepsilon \\
E\left[\bar{Y^1}_{t > T_0} - \bar{Y^1}_{t \le T_0} | D = 1 \right] &= [\beta_0 + \beta_1 + \beta_2 + \beta_3] - [\beta_0 + \beta_1] = \beta_2 + \beta_3 \\
E\left[\bar{Y^0}_{t > T_0} - \bar{Y^0}_{t \le T_0} | D = 0 \right] &= [\beta_0 + \beta_2] - [\beta_0] = \beta_2 \\
ATT &= [\beta_2 + \beta_3] - [\beta_2] = \beta_3
\end{align}
$$

## Linear model

```{r}
tidy(lm_did <- lm(y ~ treat*post,
                  data=hhf))
```

## Linear model, econ-ish specification with <span style="color:red">unit and time fixed effects</span>

$$
\begin{align}
y_{it} &= \beta_0 + \beta_1 \mbox{TREAT} \times \mbox{POST} + \alpha_i + \gamma_t + \varepsilon_{it} \\
\end{align}
$$

## Linear model with unit and time fixed effects

```{r echo=TRUE}
lm_did <- lm(y ~ tx_post + t_f + colony,
                  data=hhf)
```

```{r}
tidy(lm_did) 
```

## Difference in differences

* **Assumes** the pre-post difference in the control group is a reasonable surrogate for the counterfactual condition of the test group; 

* **Traditionally**, we would say the assumption is that the test and control groups' longitudinal trends in the pre-intervention period are parallel. See Health Policy Data Science Lab's website for a nuanced discussion. 

* Also assumes common <span style="color:red">shocks</span>. 

* **Accounts for** baseline differences in outcomes between the two groups and shocks unrelated to the intervention that affected both groups

* **Does not account for** Shocks unrelated to the intervention that only occurred in one group or differences between groups in the longitudinal trajectory before the intervention

## Assumptions

* "Parallel trends"?
* "Counterfactual assumption"?

## Parallel trends

```{r}
lm_pt <- lm(y ~ t*colony,
   data=hhf %>% filter(post==0))

hhf_proto +
  # Predicted pre-intervention
  geom_smooth(data = hhf %>% filter(post==0) %>%
                mutate(y = predict(lm_pt, 
                                   newdata=.)),
              aes(group=colony, color=colony),
              method="lm", formula="y ~ x") 
```

## Test for parallel trends

$$
\begin{align}
  Y = \beta_0 + \beta_1 t +\beta_2 \mbox{TREAT} + \beta_3 \mbox{TREAT} \times t + \varepsilon
\end{align}
$$

```{r}
lm_pt <- lm(y ~ t*treat,
   data=hhf %>% filter(post==0))
summary(lm_pt)
```

## 

![](gfx/violation.jpg){height=250}

* Match or weight by propensity score — but don't include the outcome
* Explicitly model the difference between trends — but misspecification can introduce bias
* Choose a better control group
* Give up. Go do a different study.

## Raise your hand if you just love all of this.

Let's conduct our analysis conditional on getting $p > 0.05$ on a hypothesis test where the thing we're trying to show isn't a thing is set up as the null hypothesis. 

## Do you have a better idea?

* Think more carefully about how the ATT is defined; "the test of parallel trends is neither necessary nor sufficient."
* Equivalence testing (Hartman and Hidalgo 2018)
* Model specification that accounts for non-parallel trends (Bilinski and Hatfield 2019)
* Extrapolate the deviation from the pre-intervention period into the post-intervention period (Rambachan and Roth 2019)

# Fixed effects

## Colonies

```{r}
colonies %<>%
  mutate(t_f = factor(t),
         tx_post = treat==1 & post==1)

(colonies_proto <- ggplot(data=colonies,
       aes(x=time, y=y)) +
  geom_point(aes(group=colony, color=colony),
             alpha=0.4, size=2.5) +
    annotate(geom="text",
           x = 0,
           y = min(colonies$y) - 11, 
           label=" Pre",
           hjust=0, 
           family = "xkcd") +
    annotate(geom="text",
           x = intervention,
           y = min(colonies$y) - 11, 
           label=" Post",
           hjust=0, 
           family = "xkcd") +
  geom_vline(xintercept = 31) +
  scale_y_continuous(name="Monthly operating costs, millions") +
  scale_x_continuous(breaks=seq(0, 60, 10)) +
  xkcdaxis(range(colonies$time), range(colonies$y)) +
  theme(legend.title = element_blank(),
        legend.box.background = element_rect(colour = "black"),
        legend.position = c(0.2, 0.7)) +
  guides(fill=guide_legend(ncol=2)))
```

## Three methods

* Difference score
* Unit and time dummy variables
* De-meaning

## Notation

$$
y_{it} = \mu_t + \beta \mathbf{x}_{it} + \gamma \mathbf{z_i} + \alpha_i + \varepsilon_{it}
$$
$$
\begin{align}
  \mu_t &~ \mbox{ is a time-varying intercept} \\
  \mathbf{x}_{it} &~ \mbox{ is a vector of time-varying covariates} \\
  \mathbf{z}_{i} &~ \mbox{ is a vector of time-invariant covariates} \\
  \beta, \gamma &~ \mbox{ are coefficients} \\
  \mathbf{\alpha}_i &~\mbox{ is time-invariant error for individuals} \\
  \varepsilon_{it} &~ \mbox{ is error for each individual at each time}
\end{align}
$$

## Two periods

$$
\begin{align}
  y_{i1} & = \mu_1 + \beta \mathbf{x}_{i1} + \gamma \mathbf{z}_i + \alpha_i + \varepsilon_{i1} \\
  y_{i2} & = \mu_2 + \beta \mathbf{x}_{i2} + \gamma \mathbf{z}_i + \alpha_i + \varepsilon_{i2} \\
  y_{i2} - y_{i1} &= (\mu_2 - \mu_1) + \beta(\mathbf{x}_{i2} - \mathbf{x}_{i1}) + (\varepsilon_{i1} - \varepsilon_{i2}) \\
  \Delta y_i &= \Delta \mu + \beta \Delta \mathbf{x}_i + \Delta \varepsilon_i
\end{align}
$$

* $\alpha_i$ and $\gamma \mathbf{z}_i$ go away
* Cannot estimate $\gamma$


## Two-period data, many colonies
```{r include=FALSE}
colonies_2p <- colonies %>%
  group_by(colony, treat, post) %>%
  summarise(colony_effect = mean(colony_effect),
            weather_index = mean(weather_index),
            y = mean(y)) %>%
  ungroup() %>%
   mutate(tx_post = as.numeric(treat==1 & post==1)) %>%
   select(colony, treat, post, tx_post, weather_index, y)
```

```{r}
colonies_2p
```


## Calculate differences

```{r echo=TRUE}
(colonies_diff <- colonies_2p %>%
       pivot_wider(names_from = post,
                   values_from = c(tx_post, weather_index, y)) %>%
       mutate(delta_y = y_1 - y_0,
              delta_txpost = tx_post_1 - tx_post_0,
              delta_weather = weather_index_1 - weather_index_0) %>%
   select(colony, starts_with("delta")))
```

## Difference score method

$$
\Delta y_i = \Delta \mu + \beta_1 \Delta \mbox{TREAT}_i + \beta_2 \mbox{WEATHER} + \Delta \varepsilon_i
$$

```{r echo=TRUE}
lm_fe1 <- lm(delta_y ~ delta_txpost + delta_weather, data = colonies_diff)
tidy(lm_fe1)
```


## Unit and time dummy variables

$$ 
\begin{align}
y_{it} & = \beta_0 + \beta_1 \mbox{TREAT} \times \mbox{POST} + \alpha_i + \gamma_t  + \varepsilon_{it} , \\\\
\mbox{where} \\
\alpha_i & \mbox{ are colony fixed effects, and } \\
\gamma_t & \mbox{ are time fixed effects}
\end{align}
$$

## Unit and time dummy variables, 2 periods

```{r echo=TRUE}
lm_fe2 <- lm(y ~ tx_post + weather_index + colony + post, data=colonies_2p)
```

```{r}
tidy(lm_fe2) 
```

## Unit and time dummy variables, 60 periods

```{r echo=TRUE}
lm_fe3 <- lm(y ~ tx_post + weather_index + colony + t_f, data=colonies)
```

```{r}
tidy(lm_fe3) 
```

## Variance

* Key assumption in linear regression is **independent observations**. 
* Repeated observations within each colony are correlated. 
* Failure to account for this correlation will usually (but not always) bias standard errors downward, yielding optimistic precision.

## "STATA"-flavored standard errors

Eicker-Huber-White variance

$$
\begin{align}
&\frac{N}{N-K}(\mathbf{X}^\prime \mathbf{X})^{-1} \mbox{diag} [e^2_i] \mathbf{X} (\mathbf{X}^\prime \mathbf{X})^{-1}, \\
&df = N-K
\end{align}
$$

In general, clustering at a higher level accounts for nested levels of correlation

## vcovHC and lmtest

```{r echo=TRUE}
# Fit the model
lm_fe3 <- lm(y ~ tx_post + colony + weather_index + t_f, data=colonies)

# Estimate heteroskedasticity-consistent covariance matrix
vcv_lm_fe <- sandwich::vcovHC(lm_fe3, cluster=colonies$colony)

# Coeftest
lm_fe3_hc <- lmtest::coeftest(lm_fe3, vcv_lm_fe)
```

## OLS versus HC standard errors 

```{r echo=TRUE}
# OLS results
tidy(lm_fe3) %>% filter(!str_detect(term, "^colony|^t_f"))

# HC results
tidy(lm_fe3_hc) %>%
  filter(!str_detect(term, "^colony|^t_f"))

```

## De-meaning

$$
\begin{align}
y_{it} & = \beta_0 + \mathbf{\beta x}_{it} + \alpha_i + \gamma_t  + \varepsilon_{it} \\
y^*_{it} & = \beta_0 + \beta \mathbf{x}^*_{it} + \gamma_t  + \varepsilon_{it} , \\\\
y^*_{it} & = y_{it} - \frac{1}{n_i}\sum_t y_{it}, ~~~~~~~~~~~~~~~~\mathbf{x}^*_{it} = \mathbf{x}_{it} - \frac{1}{n_i}\sum_t \mathbf{x}_{it}
\end{align}
$$

## Mean deviation algorithm

```{r echo=TRUE}
lm_demean <- colonies %>%
  group_by(colony) %>%
  mutate(y_star = y - mean(y),
         tx_post_star = as.numeric(tx_post) - mean(as.numeric(tx_post)),
         weather_star = weather_index - mean(weather_index)) %>%
  lm(y_star ~ tx_post_star + weather_star + t_f,
     data=.)

# Results
tidy(lmtest::coeftest(lm_demean, 
                      sandwich::vcovHC(lm_demean, cluster=colonies$colony))) %>%
  filter(!str_detect(term, "^colony|^t_f"))
```

## Package estimatr 
```{r echo=TRUE}
tidy(estimatr::lm_robust(y ~ tx_post + weather_index,
          fixed_effects = ~ colony + t_f,
          clusters = colony,
          se_type = "stata",
          data=colonies))
```

## Benchmarking
```{r echo=TRUE}
bench <- map_dfr(1:100, 
                 function(i) {
                   tic(paste0("lm() iteration ", i))
                   # Fit the model
                   lm_fe <- lm(y ~ tx_post + weather_index + colony + t_f,
                               data=colonies)
                   
                   # Estimate heteroskedasticity-consistent covariance matrix
                   vcv_lm_fe <- sandwich::vcovHC(lm_fe, cluster=colonies$colony) 
                   
                   # Results
                   rslts <- tidy(lmtest::coeftest(lm_fe, vcv_lm_fe)) %>%
                     filter(!str_detect(term, "^colony|^t_f"))
                   tt_lm <- toc()
                   
                   tic(paste0("lm_robust() iteration ", i))
                   rslts2 <- tidy(estimatr::lm_robust(y ~ tx_post + weather_index,
                                                      fixed_effects = ~ colony + t_f,
                                                      clusters = colony,
                                                      se_type = "stata",
                                                      data=colonies))
                   tt_lmr <- toc()
                   
                   return(tibble(lm = tt_lm$toc - tt_lm$tic,
                                 lm_robust = tt_lmr$toc - tt_lmr$tic))
                 })
```

## Benchmarking
```{r echo=TRUE}
with(bench, t.test(lm, lm_robust))
```


## Comparison to mixed-effects models

Key difference: 

* Mixed effects models assume random effects are uncorrelated covariates;
* Fixed effects models assume fixed effects are perfectly correlated with time-invariant covariates, both measured and unmeasured.
  - Cannot estimate coefficients for time-invariant covariates
  - Remove potential bias from unmeasured time-invariant confounders

## Comparison to mixed-effects models

* Fixed effects are not constrained in distribution, whereas random effects are $\sim N(0, \sigma_b)$.
* Fixed-effects models base estimates of ${\bf \hat{\beta}}$ exclusively on within-subject variance, adjusting out all of the between-subject variance, whereas mixed-effects models utilize both sources of variance, which can yield much smaller standard errors. 
* Bias-variance trade-off. 
  



# Linear probability models

## Controversy

> "There's nothing wrong with OLS."  — John McConnell

> "OLS is frequently a biased estimator and almost always an inconsistent estimator of the LPM." — Horrace and Oaxaca

> "The LPM won’t give the true marginal effects from the right nonlinear model. But then, the same is true for the “wrong” nonlinear model! The fact that we have a probit, a logit, and the LPM is just a statement to the fact that we don’t know what the “right” model is. Hence, there is a lot to be said for sticking to a linear regression function as compared to a fairly arbitrary choice of a non-linear one!" — Steffen Pischke

## Practicalities

* A link function can **dramatically** increase the complexity of a difference-in-differences model.
* In my own experiments, predicted probability from a logistic model has always been within a tiny margin of the prediction from the corresponding LPM.
  - But I always check. 
* Bias increases as the proportion of predicted probabilities outside [0,1] increases.
* Consider the PI, the target journal, and the audience. 

## Risk of alien infestation

![](gfx/freehugs.png){height=400}

```{r}
set.seed(5324)
aliens <- colonies %>%
   mutate(logit_infest = -2.5 - 0.2*colony_effect + 0.2*t +0.4*weather_index - 4*treat*post,
          prob_infest = arm::invlogit(logit_infest),
          infest = rbinom(n=nrow(.), size=1, prob=prob_infest))

# aliens %>% tabyl(treat, infest) %>% adorn_percentages("row") %>% adorn_pct_formatting() %>% adorn_ns() %>% adorn_title()
# summary(aliens$prob_infest)  
```

## LPM 

```{r echo=TRUE}
tidy(lpm_infest <- lm(infest ~ treat*post + t + weather_index, 
                      data=aliens))
```

## Logit
```{r echo=TRUE}
tidy(glm_infest <- glm(infest ~ treat*post + t + weather_index, 
                       data=aliens,
                       family=binomial(link=logit)),
     exponentiate=TRUE)
```

## Predicted probabilities
```{r}
colonies %>%
  mutate(pred_infest_lpm = predict(lpm_infest),
         pred_infest_glm = predict(glm_infest, type="response")) %>%
  ggplot(aes(x=pred_infest_glm, y=pred_infest_lpm)) +
  geom_point() +
  geom_abline(intercept=0,slope=1) +
  labs(x="GLM", y="LPM") +
  theme_bw()
```


## Fun stuff to read

1. [Health Policy Data Science Lab's Difference-in-Differences page](https://diff.healthpolicydatascience.org/)
1. Lindner S, McConnell KJ. Difference-in-differences and matching on outcomes: a tale of two unobservables. Health Serv Outcomes Res Method. 2019;19(2-3):127-144. doi:10.1007/s10742-018-0189-0
1. Bilinski A, Hatfield LA. Nothing to see here? Non-inferiority approaches to parallel trends and other model assumptions. arXiv:180503273 [stat]. Published online January 16, 2020. Accessed April 30, 2020. http://arxiv.org/abs/1805.03273
1. Rambachan A, Roth J. An Honest Approach to Parallel Trends. Working paper. 
1. Revisiting the Difference-in-Differences Parallel Trends Assumption: Part I Pre-Trend Testing. Accessed November 13, 2020. https://blogs.worldbank.org/impactevaluations/revisiting-difference-differences-parallel-trends-assumption-part-i-pre-trend

## More fun stuff to read


1. Revisiting the Difference-in-Differences Parallel Trends Assumption: Part II What happens if the parallel trends assumption is (might be) violated? Accessed November 13, 2020. https://blogs.worldbank.org/impactevaluations/revisiting-difference-differences-parallel-trends-assumption-part-ii-what-happens
1. Horrace WC, Oaxaca RL. Results on the bias and inconsistency of ordinary least squares for the linear probability model. Economics Letters. 2006;90(3):321-327. doi:10.1016/j.econlet.2005.08.024
1. Whether to probit or to probe it: in defense of the Linear Probability Model. Accessed November 13, 2020. https://blogs.worldbank.org/impactevaluations/whether-to-probit-or-to-probe-it-in-defense-of-the-linear-probability-model





.